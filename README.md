[LLM & AI深度学习全栈技术笔记.md](https://github.com/user-attachments/files/22004977/LLM.AI.md)
# 第零章 前言

这篇笔记是我在新加坡南洋理工大学人工智能专业读研期间，从互联网上收集的学习资料整理而来，加上了自己从学术界转向业界时对人工智能底层原理和当前大模型应用的一些思考和反思，希望能为未来有志于从事产品和算法的朋友们提供一点参考。部分理解尚浅，还望海涵。内容长达数十万字，但在 AI 领域日新月异的文献面前是远远不够的。附录C记录了笔者推荐阅读的一些论文和阅读笔记，有兴趣的读者可以参考。

第零章先从外界角度唠一些碎碎念；第一章主要探讨经典深度学习理论；第二三章介绍现代深度学习应用成功的经典模型；第四章先后介绍大语言模型的基本原理、开源大模型架构解析、微调方法；第五章是总结与展望，附录部分还简单介绍了相关的数理理论。想看干货的朋友直接可以跳到自己感兴趣的章节，刚入门的同学建议按序阅读。

## 产品 or 算法

这个问题我也尝试探索了很久，主要有如下几个考量：

1. 作为在校学生能接触到的实际业务实在不多
2. 现在C端产品赛道护城河不深，C端还没有出现一个统一的形态或者范式，很多公司先后布局其实并没有太大差别。是蓝海，也是死海。个人认为这种问题的本质原因在于，目前交互方式的发展过于单一，仅凭输入文字或调用 API。以后哪个产品能率先做到低延迟的多模态交互，哪个产品就更容易占领用户心智。
3. B端由于需求继承，产品形态已经相对较为固定，底层是从ERP、工业互联网发展而来的工业大模型，个人了解不多所以不再赘述。

总结下来，还是看个人性格，外向+愿意接受挑战+不害怕失败，那就大胆去做产品。我做过工业平台，也做过社媒用研，体会下来自己内在还是务实冷静更多一些，所以选择继续深耕算法路线。但是无论如何，希望大家尽量能带着产品思维做算法，或者带着算法思维做产品。笔者相信科技与人文一定是相辅相成的，否则到迭代后期容易陷入局部最优和信息茧房。这一点是我在很多高层管理者身上看到的，所以希望大家在学习的过程中保持一个开放的心态。因此，本篇路线后面会在介绍底层原理的同时尽可能带入一些产品视角。

## AI(智能)与大模型的区别

传统学术视角下，AI的目标是模仿人类大脑的思考方式。神经网络探索出了一种比较好的模拟方式，即模拟大脑中的神经元连接和交互活动，但这种模拟信息传递的底层逻辑是非常直接且简陋的（**详见MLP**）。即便如此，在宏观模型架构上采取一定调控（有[归纳偏置](##归纳偏置(inductive bias)))，加上**scaling law**，也可以造出一个庞大的机器系统——大语言模型(LLM)，你可以说它是第三次工业革命以来人类智慧的结晶，因为它涵盖了几乎所有互联网上“有用的”数据，并且可以基于此做出超乎普通人预期的反馈；你也可以说它是“伪智能”，因为它仅仅是足够人造(Artificial)。**待补充**

如今我们大部分人，包括业界的算法工程师们做的工作，就是如何让这个伪智能，在某一场景下更接近智能(但无法达到)以作为助手或工具帮助提升效率。而学术界更需要探索的是，如何让 LLM 在综合场景下，更接近通用智能(AGI)。

## 怎样理解“学习”

学习简单来讲就是从一些经历中，得到新的能力或者认知，这种能力或认知能够被用于其他更泛化更普遍的任务。

数据 -> 信息 -> 知识 -> 智慧

**无监督学习**、**有监督学习**

## 怎样理解数据

你一定听过“大数据”这个词，但背后支撑其实是基础设施infra（数据仓库、数据湖）、算力、模型和算法。在AI时代，如果你不从事infra相关，“大数据”在我看来其实是一个很不负责任的表述，不仅因为它不仅把四个东西混为一谈，更因为它对数据流转底层逻辑的简化和忽略。

就好比做菜这一个流程，需要先买菜、放到冰箱里存储起来，要用的时候再拿出来洗菜、切菜，最后一部才是下厨。做infra的同学负责之前的部分，做算法的同学只负责下厨这个部分，要研究出一个好的模型，就像研究一个好的菜谱，清楚数据是如何一步步产生反应，最后得到想要的结果。此外，做产品的同学，就像是餐馆的老板，清楚理解顾客的喜好需求，并和厨师沟通，宏观上来讲还要有一定资历。而做运营的同学，就像是宣传者，接触顾客的同时也需要了解隔壁家餐馆的情况（竞品调研等）。讲“大数据”，就像只讲“菜”，谁懂你在说哪一环的什么东西？话说回来，做算法的同学，肯定也要知道有哪些类型的数据，他们原来是什么样，送到手里是什么样，做好了又是什么样。所以我简要介绍一下数据这个概念。

宏观上，从目前数字化的成果来看，数据大体只分为两类：**数值和文本**。图像被转化为了数值（一个个像素），语言、声音也被转化为了数值序列和高维向量(详见[表征学习](##表征学习(representative learning)))。嗅觉和味觉目前除了神经脉冲信号，还没有成熟的表示方法，未来可期。

组织形式上再细一点，可以分为结构化数据、非结构化数据、半结构化数据。这是一个比较传统的划分，并且划分的意义现在已经不大。因为现在AI利用的数据绝大部分都是 `.json` 形式，把所有可结构化和不可结构化的数据强行统一在 `.json` 里面。因此，人类的自然语言已经趋向从非结构化向结构化转变。

序列数据

元数据

# 第一章 深度学习与神经网络：理论基础篇

本章主要通过构建一个基本的神经网络MLP，帮助大家熟悉理解深度学习pipeline。阅读建议：如果是新入门的同学，建议适当忽略细节，先把握框架，再把握原理，不要有畏难心理。人工智能深似海，是人都得慢慢来。

## 表征学习(representative learning)

将一个个词，或者更确切地说，将一个个输入，表示为向量/词元之后，就便于让模型在这个空间中去学习每个词的具体映射关系，这一步也叫**表征学习**。对于*传统机器学习*，关键在于如何找到一个好的表征向量，这个表征向量我们把它叫做**特征**。怎样定义一个好的特征呢？*传统机器学习* 中有以下这些基本原则：

* **判别性 Discriminativeness**：好的特征能够区分不同类别的数据。不同类别的向量在表征空间中应该有较远的距离。**详见对比学习**
* **敏感性 Sensitive and Invariant**：好的特征应该对任务无关的因素不敏感，而对任务无关的因素更敏感。比如在分类问题中，一个好的特征不应该随位置、光线的变化而变化。*详见 SIFT*
* **迁移性 Transferability：**在一个任务中学习到的向量表示，能被迁移到其他不同任务或数据集。

而在理论上，深度学习不需要手动构建特征，能够**自动学习**各种特征组合（至于为什么，详见本章后文），但我们**仍需要知道什么是好的特征**，这在强化学习和对比学习中比较重要。其实，现在大模型的瓶颈之一也在于此，虽然学习了到海量的特征，但这些特征的质量和表征效率并不高。解决方法是在预训练阶段加入更多专家知识(domain-field knowledge)，在后训练阶段引入强化学习(详见RLHF)。

## **监督学习与无监督学习**

**回归**

回归任务是预测一个连续值。回归模型的目标是学习输入特征 𝑋 到输出变量 𝑌 之间的映射关系，𝑌 是连续的数值。

常见回归任务有：房价预测、股票价格预测（输入市场指标，输出未来股价）、天气温度预测（输入历史天气数据，输出未来温度）
常见回归模型有：线性回归（Linear Regression）、决策树回归（Decision Tree Regression）、随机森林回归（Random Forest Regression）、支持向量回归（SVR）、神经网络回归（MLP, Transformer 等）

**分类**

监督学习

无监督学习

半监督学习

自监督学习

## 神经网络

明确了输入表征和学习任务，接下来就是用模型学习了。传统机器学习中用到的模型可能有很多，更偏向统计和数学；但深度学习模型，基本都离不开神经网络。神经网络顾名思义，是由一个个神经元连接组成的一片网络。采用神经网络也与人工智能的目标契合：模拟人类的大脑。

先来看我们用计算机构造的一个神经元：

把多个神经元连在一起，组成一层神经网络：

可以看出当前这样模拟的，无论是一个神经元，还是一层神经元，其实是非常简单的，只是一种数学上的线性变换。这距离生物学脑科学中真实的神经元还有很明显的差距。

### 通用逼近定理与激活函数

具体数学原理就不阐述了，在这里我们只说结论：**一个具有足够隐藏单元的前馈神经网络（通常是单层MLP）可以逼近任何连续函数，即在一定条件下，神经网络可以用来表示任何复杂的数学映射关系。**

写到这突然回想起我本科一门课程的期末考试，其中有一道题是回答为什么理论上神经网络可以逼近任何函数？这确实是一个很好的问题，下面我提供一些直观上的理解。

线性模型（如$f(x) = kx$）无法逼近非线性函数(如$q(x) = ax^2+bx+c$ )，因为线性模型的输出（$kx$）是输入（$x$）的线性变换。那么如何产生非线性呢？答案是只能通过非线性函数自身产生。如果我们直接设定目标的非线性形式，那么将变成拟合过程而非学习过程了。如果我们能合理组合**多个**非线性映射，就有可能逼近任意复杂的函数。那么有如下两个问题：1.如何产生多个非线性组合？2.如何组合？神经网络的结构正好提供了问题1所需的场所，只要在每个神经元中添加一个非线性函数，下一层就是当前层的多个非线性组合；而深度学习范式则负责解决问题2，通过可学习的参数来决定如何组合。我们把神经元中添加的非线性函数叫做**激活函数**。

答案是只要隐藏层足够大，前馈神经网络可以逼近任何连续函数。但是我们一般不这么做，而是**加深网络**，因为虽然理论上单个隐藏层已经足够，但更深的网络（深度学习）能更高效地表示复杂函数，而且更容易计算和优化。

### **不同形式的激活函数**

这里列举一些常见的激活函数，我们这一章会用到的是 $sigmoid$ 和 $softmax$ 。其他的后面用到会具体再讲，这里先给出公式。

#### sigmoid

$$
sigmoid(x) = \sigma(x) = \frac{1}{1 + e^{-x}}
$$



#### tanh

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$



#### ReLU

$$
\text{ReLU}(x) = \max(0, x)
$$

#### Softmax

$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$

常用于多分类。主要作用是将输出转换为概率分布。

top-k

### 模型搭建：多层感知机 MLP

把一层神经元堆叠到多层，加深网络，就是所谓的多层感知机MLP。下图右侧的MLP，由一个**输入层**，两个**隐藏层**和一个**输出层**组成。输入层的维度大小（神经元数目)`input_size = 10`，以此类推`hidden_size1 = 12, hidden_size2 = 12, output_size = 10`。每一层都是**全连接(Dense)**的。<img src=https://raw.githubusercontent.com/lostboiiii/picgo/main/img/MLP.jpeg alt="MLP
" style="zoom: 67%;" />

有没有不是全连接的神经网络呢？当然有。小到 CNN，大到 Deepseek 的 MoE 架构中，都有不是全连接的地方，这点我们后面再做讨论。

## 训练 pipeline

在有了模型之后，如何学习呢？人类的学习方式从文字中提取信息，而在深度学习中，数据就是“石油”，模型从数据中探索挖掘规律。所以在训练过程中需要不断地喂给模型数据，而且这个过程会重复很多次，直到模型表现达到收敛。**梯度下降**是深度学习中最主流的优化算法。

### 数据预处理

异常值（也叫离群值）

缺失值

缩放变换

min-max scale

归一化

### 前向传播

ffn

### 误差与损失函数

loss function

### 反向传播与优化算法（梯度下降）



### 参数和超参数



## 验证和测试

验证集、测试集

交叉验证 cross-validation

k-折交叉验证

### 鲁棒性robustness 和 不变性invariance

By definition, good invariance is application dependent.

## 方差-偏差平衡(bias-variance trade off)

回想一下你刷b站时候。

## 归纳偏置(inductive bias)

这个词中文翻译的比较晦涩，叫归纳偏置。每种模型会都有一定的偏差（注意不是误差），inductive bias特指因为**模型的假设能力不足**而导致的偏差。比如，使用线性模型拟合非线性的函数会导致高偏差（欠拟合）。深度神经网络的inductive bias可以简单理解为某种模型结构上的缺陷导致的**天生的偏差**。

例如，普通卷积神经网络在序列数据任务上表现一般，但它在图像处理上特别有效，这就是一种归纳偏置的体现，因为CNN的架构天然就适合处理具有空间结构的数据。详见 [3.1 CNN](##3.2 CNN)

**问题：如何降低归纳偏置？**

# 第二章 深度学习：进阶篇

本章会总结一些深度学习中的疑难点，并进一步介绍从2016年深度学习元年到2020年大模型元年之间，这一阶段的**学习范式上和数据利用上**的转变——从有监督到无监督，从判别式到生成式，从少量数据到全量数据，深度学习”不再传统“。



## **2.2 预训练和迁移学习**

预训练模型的主要动机是从大量无标注数据中学习丰富的知识，来提高后续模型在更小规模、更集中的数据集上的性能，即**迁移学习(transfer learning)**的思想。

从任务角度来看，预训练范式将训练任务分成共性学习和特性学习两个步骤。其中共性学习是在大规模无标注数据上进行的**无监督训练**，特性学习是在特定任务小样本数据集上进行的**有监督微调(supervised fine-tuning, SFT)**。

从参数角度来看，引入预训练模式后的模型参数不再是随机初始化，而是**直接导入在预训练任务过程中获得的权重**。比如，预训练语言模型的初始参数是从语言模型的生成训练过程中学习而来。

<img src="https://raw.githubusercontent.com/lostboiiii/picgo/main/img/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F.png" alt="预训练范式" style="zoom: 36%;" />

## **2.3 强化学习 RLHF**

后训练阶段引入强化学习，但这么做的底层逻辑确实有点朴素，其中不乏一些顶级学者比如Yann LeCun并不看好强化学习的未来。我个人认为，后训练这个阶段长期来看是能够优化甚至缩减掉的（底层逻辑与AGI相悖），但毫无疑问的是，当下人类需要用强化学习来解决现实问题和应用。因此在底层架构(Transformer-based)有突破之前，强化学习是一种*必要的妥协*。

## 2.5 判别式与生成式

其实这是**建模思路上**很重要的一个概念区分，很多人并没有搞清楚和重视这两种学习范式。*没有取巧的办法，最好严格按照定义理解。*

判别式模型，是直接对 **后验概率** $P(y \mid x)$ 建模。在监督学习中，它的目标是**学习类别标签**`label`**与输入特征之间的映射关系**。注意这个类别标签不一定是类别，而是我们说的`ground truth`，即真实值，它可以是分类问题中的类别，也可以是回归问题中的数值；而在无监督学习中，判别式模型的目标是**学习一个决策函数（决策边界）$f(x)$**。

而生成式模型，是对 **输入数据的联合概率分布 $P(x,y)$ **进行建模，然后再从中采样，**目标是生成新的样本或者数据**。注意，生成式模型可以用于分类，因为得到 $P(x,y)$ 后，可以根据贝叶斯定理计算出后验概率 $P(y \mid x)$ ，再根据这个后验概率进行分类。详细的概率理论介绍见 [附录](## 3.判别式与生成式)。

可以看出，生成式模型的建模思路更广，而判别式模型的建模思路更精。显然，**大模型是生成式的**，在算力和数据充足的情况下，大模型试图对所有输入数据（整个数字世界）的概率分布进行建模。而要达到通用人工智能AGI，目前来看确实只能采用生成式的建模思路。

## 2.4 数据增强

data augmentation

# 第三章 语言模型(NLP)

本章主要介绍对文本序列建模的深度学习模型(NLP模型)，他们是大语言模型的前身和基础。

## 2.1 词表示：Word2Vec

我们需要将自然语言表示为计算机能够理解的形式，这个过程叫做词表示（word representation）。传统表示方法（比如词相似度、独热编码和上下文表示）都是基于统计和词频，会造成空间稀疏，表征效率较低。对此，**词嵌入**（也叫词向量word embedding）提出了一种分布式表示方法：建立一个稠密的低维的空间，尝试将每个词映射到该空间并用对应向量表示。但是这种做法需要先将句子分成词，将其分成一个个**词元** `token`，再将词元表示成词向量，这也是目前大语言模型采用的分词表示方法。注意：词元不一定是实际语义上的词，而是“最小组成单元”，比如英语的组成单元是词，而汉语的组成单元是字。*所以理论上，将英文语言模型和中文语言模型统一起来是做了妥协的。*

实际上，一个 `token` 可能对应句子中的一个单词，图像中的一个补丁(一个小块)，甚至可以是蛋白质中的一个氨基酸。它们都是“最小组成单元”。

Word2Vec是无监督学习。学习词向量，就是学习隐藏层的权重表示。

给定一个句中的特定单词（输入词），随机选择一个窗口大小附近的单词（比如前三个和后三个）。Word2Vec模型的输出是词汇表（所有词汇）中每个单词作为这个输入词的“附近单词”的概率。显然，这个概率将与（在所有训练集中）在输入单词附近找到每个词汇的概率有关。例如，如果你给训练过的模型提供输入词“中国”，那么输出结果中对于像“北京”或者“熊猫”这样的词语的概率，一定比不相关的词语如“企鹅”和“袋鼠”的输出概率高。

**静态表征与动态表征**



## 2.4 自回归与next-token-prediction

之前有讲到，生成式模型尝试对所有训练数据的联合概率分布进行建模。而自回归模型(Autoregressive Model)是生成式模型的一种特例，它预测的新目标值是基于前面若干个已生成值的。在语言模型中，就是根据前文出现过的词)预测下一个词，计算可能出现词的条件概率，以及最终一组词语能否成句的联合概率。**这种做法的基本假设是，一个未来词汇出现的概率仅决定于前文词汇。**基于这个假设，一组词组成一个语句的联合概率可被表示为句中每一个词基于其前所有词条件概率的乘积。
$$
p(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} p(x_t \mid x_1, x_2, \dots, x_{t-1})
$$

Illya 曾在与黄仁勋的对谈中指出，当代大语言模型预测下一个词时，实际上理解了文本背后的现实世界过程和情感等信息，形成了「**世界模型**」。

## 2.3 Transformer

Transformer是如今大语言模型的底层模块，要搞懂大模型先必须搞懂Transformer。有点复杂但我们慢慢来，话不多说先上图。每个模块和每个箭头都是有意义的。

<img src="https://raw.githubusercontent.com/lostboiiii/picgo/main/img/20250202134946799.png" style="zoom: 80%;" />

用文字来总结一下这个架构：Transformer 由多个堆叠的**编码器(encoder)**和多个堆叠的**解码器(decoder)**两个部分而成。对输入添加**位置编码**后，进入每个编码器。在编码器中计算**多头注意力(MHA)**，穿插**残差连接**和**正则化**后，进入**前馈网络(MLP)**。

用数学语言表示就是：。。。

有很多名词听不懂没关系，后面每一小节是对编码器和解码器里每个模块的计算流程和设计思路的解析。



### 2.3.1 位置编码 Positional Encoding

无序变为有序。

RoPE

### 2.3.2 自注意力/交叉注意力 self-attention/cross-attention

相信各位肯定听过`attention`这个最出圈的概念，发展至今已经出现了各种其他的花式注意力机制，不同模型的实现原理也稍有不同，详见第四章的模型分析部分。我们先讲原始版本的。

`attention`提出的基本想法是，并不是所有的输入数据都是有用的。就像要判断一辆车是不是宝马，通常我们不会去看它的窗户、轮胎等不重要的部分，而是去看它的车标。所以要得出结果(输出)，其实不用关注整体，而只需要关注输入中重要的部分即可。这就是大脑对注意力的分配，我们会相对更关注重要的，相对忽略不重要的。如何量化这种分配呢？Transformer给出的解法就是**注意力分数**`attn_score`。重要的向量的权重更大，不重要的向量权重更小。有的同学可能会问，我怎么知道哪些重要，哪些不重要呢？答案是没有人天生就会很好地分配注意力，这个能力是在学习的过程里学来的。这里的权重也是一样，是**可学习的超参数**。

我们这里先介绍**自注意力机制** ` self-attention`。计算步骤如下：

1. 对于句子中的每一个`token`，分别与三个权重矩阵 $W_Q,W_K,W_V$ 相乘，生成 $q,k,v$ 三个向量。

	Query（查询）：代表当前关注的位置（如当前处理的`token`）。

	Key（键）：代表序列中所有位置的标识，因此$d_k$一般对应为`token`的数量。

	Value（值）：存储实际的信息。论文中默认的三个向量维度 $d$=64，现实应用中需要更大才能起效。

$$
Q=W_qX\\
K=W_kX\\
V=W_vX\\
$$

2. 计算**相似度分数**，以句子中的第一个`token`为例，拿每个`token`的 $k$ 去点积 $q_1$ ，这个分数决定了**在编码第一个词的过程中有多重视句子的其他部分**
	$$
	QK^T = 
	\begin{bmatrix} 
	q_1 \\ 
	q_2 \\ 
	\vdots \\ 
	q_{n} 
	\end{bmatrix} 
	\begin{bmatrix} 
	k_1^T & k_2^T & \cdots & k_{n}^T 
	\end{bmatrix}\\
	$$

3. 将相似度分数除以一个缩放因子，一般采用**缩放点积模型**的情况下是键向量维度的平方根$\sqrt{d_k}$

4. 用 $softmax$ 将相似度分数归一化，得到**注意力分数**，表示句中每个单词对编码当前位置(当前`token`)的贡献，注意包括自己对自己的贡献

5. 对 $v$ 点乘 $softmax$ 分数，加权求和得到**最终值**(输出)，加权的目的就是希望关注语义上相关的单词，弱化不相关的词

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) \cdot V\\
$$

6. 对所有 $n$ 个 `token` 重复，即从向量拓展到矩阵

注：

- **’‘自’‘(Self)**表示的是 $Q,K,V$ 都来自同一个输入序列 $X$，宏观表现为自己对自己的注意力得分。
- $Q$ 表示当前处理的`token`，当 $Q$ 来自一个序列，而 $K,V$ 来自另一个序列时，就是`cross-attention` 机制，例如解码器部分使用的就是**交叉注意力**，详见[2.3.2](#2.3.2)
- `self-attention`加权求和后的值 $V$是内部的元素之间的相互注意力，而`cross-attention`则表示不同序列中的两个`token`之间的依赖关系。

 **常见问题：为什么要除以$\sqrt{d_k}$** ？

>假设 $Q$ 和 $K$ 中的向量都服从高斯分布，即均值为0，方差为1，那 $Q K^T$均值为0，方差为 $d_k$。此时若直接做$softmax$，分母是指数的相加，分母较大，会导致 $softmax$ 的梯度变小，参数更新困难。除以一个缩放因子，可以使梯度更稳定。

**另一个重要的问题：为什么最终值是V和注意力分数的加权求和？**

>最终值是当前查询 $Q$ 在考虑了所有相关上下文信息后的综合表示

### 2.3.2 多头注意力 multi-head attention<span id='2.3.2'> </span>

上述注意力的计算也可以被简化为下图，这就是注意力的一个**“头”(head)**。可以看到，进入编码器的输入是一次性的，虽然被拆分成了`token`，但所有`token`全部一次性一起进行计算。在实际使用中，一般叠加**多头注意力**达到并行稳定的效果。实现方法也很简单，只需要在矩阵中**拼接(concat)**即可。实际应用中`d_head = d_model / d_k`



<img src="https://raw.githubusercontent.com/lostboiiii/picgo/main/img/20250218175033146.png" style="zoom: 67%;" />

**常见问题：为什么要用多头注意力？**

xxx

### 2.3.4 层归一化 LayerNorm



### 2.3.6 残差连接

残差连接是由`ResNet`（一种视觉模型）首先提出的，详见第四章___。残差网络的作用：缓解层数加深带来的信息损失，采用两个线性变换激活函数为`ReLU`，同等层数的前提下残差网络也收敛得更快



### 解码器

解码器与编码器略有不同。首先，当模型预测下一个单词时，它需要知道句子从哪里开始。因此，就像在编码器中使用 <EOS> 标记标记标记句子的结束一样，这次在解码器中将使用 <SOS> 标记标记标记句子的开头。<SOS> 代表序列的开始。

### 2.3.5 KV Cache



## 2.4 BERT

1. BERT 的基本任务：mask language model 和 next sentence prediction

2. BERT的MASK方式：选择15%的token进行遮盖，再选择其中80%进行mask，10%进行随机替换，10%不变

优点：

1）被随机选择15%的词当中，以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；

2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。

缺点：针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。

3. Bert的构成：由12层Transformer Encoder构成

# 第四章 视觉模型(CV)

本章主要介绍深度学习模型在计算机视觉领域的应用发展。

## 3.1 图像表示

跟文本语言类似，我们也需要将图像转化为向量，再利用模型进行学习。图片在计算机中存储的格式是RGB格式，R代表红色，G代表绿色，B代表蓝色，每张图片都是由这三个`channel`的像素值堆叠起来，底层原因是因为所有颜色都可以用三原色按不同比例调配得到。如果是灰度图像则只有一个`channel`。无论是彩色图像还是灰度图像，每个`channel`都有`w × h`个像素值，代表横向（w表示宽度）和纵向（h表示高度）的每一个点，而这个点在每个`channel`的像素值又由0-255中的一个数字表示，数值越大代表颜色越深。所以一个彩色图像一共有`3 × w × h`个像素值。每个`channel`的像素值我们可以很方便地用二维矩阵来表示，3个`channel`就用3个矩阵，但为了方便表示整个图像，我们可以用1个包含3个二维矩阵的三维张量来直接表示。这个三维张量的形状也是`3 × w × h`。

## 3.2 CNN

Every image pixel, no matter its location, go through the same transformation.

## 3.3 ResNet

**关于残差连接residual**

一些人认为，网络深度和时间维度是差不多的东西，只是在应用的时候取不同的名字。所以一层的LSTM可以类似于多层的参数共享的 `ResNet`。反过来说，把 `ResNet` 横过来看，“每个时间维度”的 `token` 是通过残差输入提供的。

## 3.4 ViT

Vision Transformer

# 第五章 大语言模型

4.1节讲大模型应用上的工程原理，4.2讲一些现象层面的理解(缺乏严格的理论依据)。

## 4.1 原理综述

### 4.1.1 模型训练

无监督预训练+有监督微调，迁移学习。

**less is more for reasoning**

强化学习

PPO近端策略优化

### 4.1.2 模型推理 Inference/Reasoning

**提示词 Prompt**

\**你是一个小助手......**

无论是否联网，推理阶段都不发生反向传播，不产生参数更新。也就是说，无论是你用了三天，还是用了三个月，只要厂商没有发布新的版本，你用的模型本身是不会改变的。你之所以可能感受到回复的质量越来越高，仅仅是因为你更熟练使用了 prompt 而已。

很多同学搞不明白inference和reasoning的区别，我第一次听到这两个词的时候也觉得指的是一个东西。但实际上有一些区别，总的来说，**inference强调的是前向传播，生成输出的过程；而reasoning指的是更宏观的推理，比如复杂问题的归因。**因此，Reasoning 可能需要多轮 Inference：例如 CoT 推理，会让模型先生成中间推理步骤，再最终得到答案，相当于执行了多次 inference。

此外，这么叫主要是为了在**优化方向**上将二者区分：Inference 优化主要涉及加速计算（如量化、蒸馏、缓存等）。Reasoning 优化 主要涉及提升逻辑能力（如 CoT、Prompt、SFT、RLHF 等）。

### 4.1.3 模型量化与模型压缩

quatization

### 4.1.4 自回归与next token prediction

1.  LLM 中，预测下一个 token 的范式得以广泛应用的原因之一在于，其在工程实现上相对容易，也因此在实践中取得了显著的成效。由于计算机只能处理离散的数值表示，因此需要将文本转换为向量进行数学运算，token 则是实现这种转换的最简单直接的方式
2.  然而，业内对这种用离散的符号系统来捕捉连续且复杂的人类思维的方式的诟病在于，它和人类思维的本质存在冲突，因为人类并不会以 token 的方式进行思考。人类在解决复杂任务或撰写长篇文档时，往往采用分层方法，通常先在高层次规划整体结构，再逐步添加细节。以准备演讲稿为例，人类通常不会准备好每个具体的用词，而是罗列需要在演讲中传达的核心观点和流程。即便进行多次同主题演讲，其中的具体用词、语言均可能不同，但核心观点的逻辑流程保持不变。以撰写论文为例，人类通常会准备一个整体框架，将文档分为若干章节，然后逐步细化。人们还能在抽象层面上识别和记住长篇文档各部分之间的关联。

问答范例QA SET

搜索引擎与生成式模型的区别：复制；**学习**通用规律，泛化。

## 4.2 一些很火的名词

### scaling law与double descent

规模校应。二次下降。

### 思维链

Chain of Thought

### 上下文学习 In-Context Learning



### 具身智能 Embodied Intelligence



### 幻觉



### 可解释性



自此，需要大家理解的LLM 相关概念就大致讲的七七八八了。接下来我们会深入地对几个当代 LLM 逐一分析，包括架构、训练方法、优化策略。希望你们已经做好了准备，加油。

## 4.2 GPT系列



## 4.3 LLaMA系列

RoPE 旋转位置编码

## 4.5 Qwen系列



## 4.6 DeepSeek系列

### 4.6.2 MoE

Mixture of Expert 混合专家架构

### 4.6.3 MLA

Multi-head latent Attention 多头潜在注意力机制

# 第六章 多模态

## 4.9 多模态大模型 MLLM

### 柏拉图表征假说

一些学者提出了“柏拉图表示假说”（Platonic Representation Hypothesis），**主张不同的神经网络在表示数据时的方式正在变得越来越一致**。他们认为这种收敛是朝着一个共享的现实统计模型发展的，这与古希腊哲学家柏拉图关于理想现实的概念相似。

<img src="https://raw.githubusercontent.com/lostboiiii/picgo/main/img/1738469489600.png" style="zoom: 67%;" />

有很多思路，比如对比学习。

### 变分自编码器 VAE



### CLIP-2

# 第七章 微调和评估实践

## 4.7 模型微调

llama, qwen, deepseek

### 4.7.1 低秩自适应 LoRA

**低秩自适应（low-rank adaptation, LoRA）**是目前针对预训练大语言模型的主流微调技术之一。LoRA 的基本原理是在冻结预训练权重参数的情况下，对模型加入额外的网络层，在微调阶段只训练新增的网络层参数，从而在减少训练参数的同时保留较佳的学习性能。

LoRA 的提出基于这样一种基本观察：每一层神经网络的权重矩阵 $W$ 通常具有满秩。因此，可通过 $W$ 的 **秩分解** 来避免反向传播过程中参数的全量更新。具体地，在微调过程中，除预训练好的模型权重之外，引入两个秩分解矩阵  和  作为新增权重参与前向传播运算，将两条链路的结果求和后作为本层的输出。例如，对于线性隐藏层  ，其前向计算过程将由式(2-2)更新为式(2-3)。

 



式中   ——预训练权重矩阵，  ；

​         ——零初始化矩阵，  ；

  ——高斯初始化矩阵，  ；

  ——输入向量；

  ——偏置向量。

在微调时，由于对  使用零初始化，因此  =  在训练开始时为零；在反向传播阶段，由于  被冻结不接受梯度更新，因此只更新新增的权重矩阵  和  。本质上，LoRA等效于一个简单的全连接层，可被应用于几乎所有基于神经网络的模型架构，诸如Transformer和各类LLMs。因此，LoRA 的适用范围还是很广的。

### 4.7.2 指令微调



## 4.8 模型评估

评估集

开源基准benchmark



# 结语

感谢你能看到这里。

AGI

具身智能

给岁月以文明，给时代以AI

---



# 附录A：机器学习与统计推断

以下内容较为晦涩枯燥，献给想深耕AGI，对数理统计理论要求较高的朋友。

统计推断的核心问题是：**如何从观测变量（已知变量）中，找到未知变量的分布？**

## 1.极大似然估计MLE



## 2.实证经验最小化ERM



## 3.概率论视角下的生成式模型

虽然生成式模型主要用于生成数据（如图像生成、文本生成），但它们也可以用于分类任务。核心思想是先学习数据的联合概率分布 $P(x, y)$，然后利用 **贝叶斯定理** 计算后验概率 $ P(y \mid x) $，从而进行分类：

$$
P(y \mid x) = \frac{P(x \mid y) P(y)}{P(x)}
$$
其中：
- $P(x \mid y)$  表示在类别 $y$ 下观测到数据 $x$ 的概率（**类别条件概率分布**）。
- $P(y)$ 是类别的**先验概率**（数据集中各类别的比例）。
- $P(x)$ 是所有类别下数据的**边际概率**（通常不需要显式计算）。

---

### **1. 朴素贝叶斯（Naïve Bayes）**

#### **方法**

朴素贝叶斯是一种经典的生成式分类模型，它假设 **各特征 $ x_i $ 之间相互独立**，即：

$$
P(x \mid y) = P(x_1, x_2, ..., x_n \mid y) \approx P(x_1 \mid y) P(x_2 \mid y) ... P(x_n \mid y)
$$

最终计算后验概率并选取最大概率类别：
$$
y^* = \arg\max_y P(y \mid x) = \arg\max_y P(y) \prod_{i} P(x_i \mid y)
$$

#### **优缺点**

✅ **优点**：
- 计算简单，适用于高维稀疏数据（如文本）。
- 训练速度快，适合大规模数据。

❌ **缺点**：
- 独立性假设通常不成立，在复杂任务上效果较差。

---

### **2. 隐马尔可夫模型（HMM）**
#### **方法**
HMM 也是一种生成式分类模型，常用于**序列分类**。HMM 通过学习 **状态转移概率 $ P(y_t \mid y_{t-1}) $** 和 **观测概率** $ P(x_t \mid y_t) $ 来进行分类。

---

### **3. 变分自编码器（VAE）+ 分类器**

#### **方法**
变分自编码器（VAE）是一种生成式模型，主要用于数据建模和生成。我们可以将 VAE 用于分类任务的方法包括：

1. **半监督学习**
   - 训练 VAE 来学习数据的隐变量 $z$  。
   - 在 VAE 的隐空间中训练一个分类器 $P(y \mid z) $ 进行分类。

2. **联合建模**
   
   - 直接在 VAE 结构中建模类别变量 $y$ ，使其成为隐变量的一部分：
   $$
   P(x, y) = P(x \mid y) P(y)
   $$
   - 训练后，使用贝叶斯推理计算类别概率 $P(y \mid x)$。

---

### **4. 生成对抗网络（GAN）+ 分类器**
#### **方法**
GAN 本质上是一个**生成模型**，但可以用于分类：
- 训练 **生成器 $G(z)$ ** 生成数据，使其接近真实数据。
- 训练 **判别器 $D(x)$ ** 用于区分真实和生成的数据。
- **扩展：** 可以让判别器 $D(x)$ 不仅区分真假，还可以进行分类：
  $$
  D(x) = \text{softmax} (W x + b)
  $$

**特别案例：**

- **AC-GAN（Auxiliary Classifier GAN）**：在标准 GAN 结构中加入辅助分类器，直接预测类别：
  $$
  D(x) \to [ P(\text{real} \mid x), P(y \mid x) ]
  $$
- **Semi-supervised GAN**：利用生成样本增强分类器训练。

---

### **5. 扩散模型（Diffusion Model）+ 分类器**
#### **方法**

扩散模型（如 **Stable Diffusion, DDPM**）主要用于图像生成，但也可以用于分类：
- **预训练扩散模型** 来学习数据分布。
- **提取隐变量特征**，然后用分类器（如 MLP）进行分类：
  $$
  P(y \mid z) = \text{softmax}(W z + b)
  $$
- 也可以直接训练扩散模型，让其同时进行分类任务。

---

### **6. GPT、BERT 等大型生成模型的分类应用**
#### **方法**

虽然 GPT 和 BERT 主要是**生成式语言模型**，但它们可以用于分类：
- **GPT-3/4 + Prompt Engineering**
  - 通过 prompt 设计，使 GPT 生成类别标签：
    ```
    输入: "这篇文章的情感是？选项：[积极, 消极, 中立]"
    GPT 输出: "积极"
    ```
- **BERT + 分类头（Fine-tuning）**
  - 在 `BERT` 预训练模型上加入分类层：
    $$
    P(y \mid x) = \text{softmax}(W h + b)
    $$
    其中 $h$ 是 `Transformer` 提取的文本特征。

---

### **总结：生成式模型用于分类的主要方式**

| 方法                          | 关键思想                             |
| :---------------------------- | :----------------------------------- |
| **朴素贝叶斯（Naïve Bayes）** | 直接建模 $P(x \mid y) P(y) $         |
| **HMM（隐马尔可夫模型）**     | 建模序列数据的 $P(x_t \mid y_t)$     |
| **VAE + 分类器**              | 用 VAE 隐变量学习 $ P(y \mid z) $    |
| **GAN + 判别器分类**          | 让判别器同时分类 $P(y \mid x)$       |
| **扩散模型 + 分类**           | 先生成数据再分类，或直接加入分类任务 |
| **GPT/BERT 生成式分类**       | 通过 prompt 或 fine-tuning 进行分类  |

---

# 附录B：神经网络设计

## 2.1 RNN/LSTM/GRU

RNN 是面向序列数据建模的神经网络。结构由图中所示多个重复的相同单元组成，因此叫“循环”。这个单元可按照时间步展开，在每个时间步接收:Ⅰ.当前时间步的输入 Ⅱ.上一个时间步的输出。然后加权计算得到当前时间步的输出。

<img src="https://raw.githubusercontent.com/lostboiiii/picgo/main/img/20250202122624589.png" style="zoom:80%;" />

LSTM和GRU是RNN的变种。

#### 激活函数：

LeakyReLU
$$
\text{LeakyReLU}(x) =   \begin{cases}   x, & x > 0 \\   \alpha x, & x \leq 0   \end{cases}
$$

GeLU
$$
\text{GeLU}(x) = x \cdot \Phi(x)
$$

其中，$\Phi(x)$ 是标准正态分布（均值 $μ = 0$，方差$\sigma^2=1$）的累积分布函数($CDF$)： $$ \Phi(x) = \frac{1}{2} \left( 1 + \operatorname{erf} \left( \frac{x}{\sqrt{2}} \right) \right) $$ 

Swish
$$
\text{Swish}(x) = x \cdot \sigma(\beta x) = x \cdot \frac{1}{1 + e^{-\beta x}}
$$

$ \beta $ 是***超参数***，通常取 $ \beta = 1$.



#### Transformer：



#### 底层架构：

mamba

# 附录C：LLM paper reading list(持续更新)

